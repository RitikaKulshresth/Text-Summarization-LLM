{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PDF Text Summarization and Similarity Search\n",
        "Overview\n",
        "\n",
        "\n",
        "This project provides a complete solution for extracting text from PDF documents, generating embeddings, finding similar documents based on cosine similarity, and summarizing the most relevant text using deep learning models. The code is designed to run in Google Colab, leveraging GPU capabilities for faster processing."
      ],
      "metadata": {
        "id": "FsXN_-EwSls-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Framework\n",
        "\n",
        "Libraries Used\n",
        "\n",
        "1. Transformers: A library by Hugging Face that provides pre-trained models for natural language processing (NLP) tasks such as summarization and text embedding.\n",
        "2. PDFPlumber: A Python library for extracting text and metadata from PDF files.\n",
        "3. Sentence Transformers: A library for generating sentence and text embeddings efficiently, based on transformer models.\n",
        "4. Scikit-learn: A machine learning library that provides tools for calculating cosine similarity between vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "jOTAf0C--OCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iET0STCC-0Mn",
        "outputId": "70ff87f4-4267-40df-8390-637c36fe6244"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# Create a new directed graph\n",
        "dot = Digraph()\n",
        "\n",
        "# Add nodes with labels\n",
        "dot.node('A', 'PDF Folder')\n",
        "dot.node('B', 'Extract Text with PyMuPDF')\n",
        "dot.node('C', 'Store Text Data')\n",
        "dot.node('D', 'Generate Embeddings with Sentence Transformers')\n",
        "dot.node('E', 'Find Similar Documents using Cosine Similarity')\n",
        "dot.node('F', 'Summarize Text using Transformers')\n",
        "dot.node('G', 'Output Summary')\n",
        "\n",
        "# Add edges to define the flow\n",
        "dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG'])\n",
        "\n",
        "# Render the graph\n",
        "dot.render('flowchart', format='png', cleanup=True)\n",
        "\n",
        "# Display the graph in the notebook\n",
        "dot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "K_9Ec2T2-5tZ",
        "outputId": "3fa7bdbf-4353-4354-c628-3cf626b60395"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"390pt\" height=\"476pt\"\n viewBox=\"0.00 0.00 390.17 476.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 472)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-472 386.17,-472 386.17,4 -4,4\"/>\n<!-- A -->\n<g id=\"node1\" class=\"node\">\n<title>A</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-450\" rx=\"51.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">PDF Folder</text>\n</g>\n<!-- B -->\n<g id=\"node2\" class=\"node\">\n<title>B</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-378\" rx=\"112.38\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">Extract Text with PyMuPDF</text>\n</g>\n<!-- A&#45;&gt;B -->\n<g id=\"edge1\" class=\"edge\">\n<title>A&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-431.7C191.08,-423.98 191.08,-414.71 191.08,-406.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-406.1 191.08,-396.1 187.58,-406.1 194.58,-406.1\"/>\n</g>\n<!-- C -->\n<g id=\"node3\" class=\"node\">\n<title>C</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-306\" rx=\"65.79\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Store Text Data</text>\n</g>\n<!-- B&#45;&gt;C -->\n<g id=\"edge2\" class=\"edge\">\n<title>B&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-359.7C191.08,-351.98 191.08,-342.71 191.08,-334.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-334.1 191.08,-324.1 187.58,-334.1 194.58,-334.1\"/>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-234\" rx=\"191.17\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Generate Embeddings with Sentence Transformers</text>\n</g>\n<!-- C&#45;&gt;D -->\n<g id=\"edge3\" class=\"edge\">\n<title>C&#45;&gt;D</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-287.7C191.08,-279.98 191.08,-270.71 191.08,-262.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-262.1 191.08,-252.1 187.58,-262.1 194.58,-262.1\"/>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-162\" rx=\"184.67\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Find Similar Documents using Cosine Similarity</text>\n</g>\n<!-- D&#45;&gt;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>D&#45;&gt;E</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-215.7C191.08,-207.98 191.08,-198.71 191.08,-190.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-190.1 191.08,-180.1 187.58,-190.1 194.58,-190.1\"/>\n</g>\n<!-- F -->\n<g id=\"node6\" class=\"node\">\n<title>F</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-90\" rx=\"140.28\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Summarize Text using Transformers</text>\n</g>\n<!-- E&#45;&gt;F -->\n<g id=\"edge5\" class=\"edge\">\n<title>E&#45;&gt;F</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-143.7C191.08,-135.98 191.08,-126.71 191.08,-118.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-118.1 191.08,-108.1 187.58,-118.1 194.58,-118.1\"/>\n</g>\n<!-- G -->\n<g id=\"node7\" class=\"node\">\n<title>G</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.08\" cy=\"-18\" rx=\"72.59\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.08\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Output Summary</text>\n</g>\n<!-- F&#45;&gt;G -->\n<g id=\"edge6\" class=\"edge\">\n<title>F&#45;&gt;G</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.08,-71.7C191.08,-63.98 191.08,-54.71 191.08,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.58,-46.1 191.08,-36.1 187.58,-46.1 194.58,-46.1\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x799a5c329960>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will open an upload dialogue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4cK8Ia0Vcu-6",
        "outputId": "cc5cbbf8-37f2-4991-fe3e-33e0385ff73a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ff06116-f544-405b-a445-2bcc2d90620e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ff06116-f544-405b-a445-2bcc2d90620e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.zip to data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip the uploaded zip file\n",
        "zip_file = \"data.zip\"  # Name of the uploaded zip file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/pdfs\")  # Extract to the /content/pdfs directory\n",
        "\n",
        "# Verify the directory contents\n",
        "os.listdir(\"/content/pdfs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBlvh-Jwde9e",
        "outputId": "112dcd6b-1897-4c79-cab7-107b94b47bb1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUJkXNhTc7Z3",
        "outputId": "6d27373b-6988-4844-81ad-eed76542e5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.10)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.10 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.24.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install PyMuPDF transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import pickle\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "AGWhpm5PjWYd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract Text from PDFs\n",
        "def extract_text_from_pdfs(pdf_folder):\n",
        "    text_data = {}\n",
        "    for pdf_file in os.listdir(pdf_folder):\n",
        "        if pdf_file.endswith('.pdf'):\n",
        "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            text_data[pdf_file] = text\n",
        "            doc.close()\n",
        "    return text_data\n"
      ],
      "metadata": {
        "id": "e9yfbGSYjaHq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Save Extracted Text as Pickle\n",
        "def save_text_data(text_data, output_file):\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(text_data, f)\n"
      ],
      "metadata": {
        "id": "jKb-ABe7jdMb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load Text Data from Pickle\n",
        "def load_text_data(input_file):\n",
        "    with open(input_file, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "OcJ444GEjg3J"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate Embeddings\n",
        "def generate_embeddings(text_data):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = {doc: model.encode(text) for doc, text in text_data.items()}\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "SSvBRmtSjly5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Find Most Similar Document\n",
        "def find_most_similar(query, embeddings, model):\n",
        "    query_embedding = model.encode(query)\n",
        "    similarities = {}\n",
        "\n",
        "    for doc, embedding in embeddings.items():\n",
        "        sim = cosine_similarity([query_embedding], [embedding])[0][0]\n",
        "        similarities[doc] = sim\n",
        "\n",
        "    sorted_docs = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    if len(sorted_docs) == 0:\n",
        "        raise ValueError(\"No documents found with valid similarity scores.\")\n",
        "\n",
        "    return sorted_docs[0]  # Return the most similar document\n"
      ],
      "metadata": {
        "id": "NXRxOcSZjpGN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Summarize Text\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n"
      ],
      "metadata": {
        "id": "E0j-CiOwjpJF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Run the Complete Workflow\n",
        "# Specify the folder containing your PDF files\n",
        "pdf_folder = \"/content/pdfs/data\"  # Update this path to your PDF folder\n",
        "pickle_file = \"text_data.pkl\"\n",
        "\n",
        "# Extract text from PDFs\n",
        "text_data = extract_text_from_pdfs(pdf_folder)\n",
        "\n",
        "# Check if text_data is empty and print a message if it is\n",
        "if not text_data:\n",
        "    print(\"No text data found in the specified folder.\")\n",
        "\n",
        "# Save the extracted text as a pickle file\n",
        "save_text_data(text_data, pickle_file)\n",
        "\n",
        "# ... rest of your code ..."
      ],
      "metadata": {
        "id": "BxD0fTEkjpMg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from PDFs\n",
        "text_data = extract_text_from_pdfs(pdf_folder)\n",
        "\n",
        "# Save the extracted text as a pickle file\n",
        "save_text_data(text_data, pickle_file)\n",
        "\n",
        "# Load text data from the pickle file\n",
        "text_data = load_text_data(pickle_file)\n",
        "\n",
        "# Generate embeddings for the extracted text\n",
        "embeddings = generate_embeddings(text_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASPjg_oij3Ph",
        "outputId": "cfd6fa88-f341-45dd-efb2-13113ee4f465"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"what is Multi-Head Attention?\"\n",
        "'\n",
        "\n",
        "# Find the most similar document\n",
        "most_similar_doc = find_most_similar(query, embeddings, SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "print(f\"Most similar document: {most_similar_doc[0]} with similarity score: {most_similar_doc[1]:.4f}\")\n",
        "\n",
        "# Summarize the most similar document\n",
        "relevant_text = text_data[most_similar_doc[0]]\n",
        "summary = summarize_text(relevant_text)\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTXr-DEmj3Sy",
        "outputId": "514a7945-04db-4f48-99e6-796ee981e64c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar document: NIPS-2017-attention-is-all-you-need-Paper.pdf with similarity score: 0.3023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8537 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: a recurrent neural network consists of two sub-layers, a stack of N = 6 identical layers . a single attention layer is based on a multi-head attention function . we use the decoder to compute a representation of a sequence of symbols .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b98cagFWaSXG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VITSGdRioeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4AMOpzFfmtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}